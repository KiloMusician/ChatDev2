Create Ollama AI runner CI script: 1) Check Ollama service availability on localhost:11434, 2) Validate required models are installed (qwen2.5-coder, starcoder2, etc), 3) Test model inference with sample prompts, 4) Report health metrics (response time, success rate), 5) Exit with appropriate error codes for CI pipeline, 6) Add JSON output for CI integration, 7) Include retry logic for transient failures, 8) Add verbose logging mode. Target: ollama_ai_runner.py for scripts/ci/ directory.