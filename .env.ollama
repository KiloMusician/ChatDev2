# ============================================================================
# ChatDev Ollama Integration Configuration
# ============================================================================
# This configuration enables ChatDev to use local Ollama models instead of
# OpenAI API, providing fully local AI-powered software development.
#
# Usage:
#   1. Windows (PowerShell):
#      Get-Content .env.ollama | ForEach-Object { if ($_ -match '^([^=]+)=(.*)$') { [Environment]::SetEnvironmentVariable($matches[1], $matches[2], 'Process') } }
#
#   2. Unix/Linux/macOS:
#      export $(cat .env.ollama | xargs)
#
#   3. Or source directly:
#      source .env.ollama  # Unix/Linux/macOS
# ============================================================================

# === Ollama API Configuration ===
# OpenAI-compatible API endpoint (Ollama provides OpenAI-compatible API)
OPENAI_API_KEY="ollama"
BASE_URL="http://localhost:11434/v1"

# === Model Selection ===
# ChatDev uses GPT-3.5/GPT-4 by default. Map to your best coding model.
# Recommended: qwen2.5-coder:14b for best results, qwen2.5-coder:7b for speed

# Note: ChatDev expects model names like "gpt-3.5-turbo"
# We'll need to modify the code to use Ollama model names directly
# For now, Ollama will translate OpenAI model names to local models

# === Performance Tuning ===
# Adjust these based on your hardware and requirements
OLLAMA_NUM_PARALLEL=2        # Number of parallel requests
OLLAMA_NUM_GPU=1              # Number of GPUs to use
OLLAMA_NUM_THREAD=8           # CPU threads for inference

# === Model-Specific Configuration ===
# Primary coding model (used for most agent interactions)
CHATDEV_CODING_MODEL="qwen2.5-coder:14b"

# Fast model (for simple tasks, refactoring suggestions)
CHATDEV_FAST_MODEL="qwen2.5-coder:7b"

# Reasoning model (for complex architectural decisions)
CHATDEV_REASONING_MODEL="gemma2:9b"

# === Optional: Model Fallback Chain ===
# If primary model fails, try these in order
CHATDEV_FALLBACK_MODELS="qwen2.5-coder:14b,qwen2.5-coder:7b,codellama:7b"

# === Logging & Debugging ===
CHATDEV_LOG_LEVEL="INFO"      # DEBUG, INFO, WARNING, ERROR
CHATDEV_ENABLE_COST_TRACKING="false"  # Disable cost tracking for local models

# ============================================================================
# Notes:
# - Ollama must be running: ollama serve
# - Verify models are available: ollama list
# - For best results, use models with at least 7B parameters
# - qwen2.5-coder is specifically trained for code generation
# - Larger context windows allow for more complex projects
# ============================================================================
